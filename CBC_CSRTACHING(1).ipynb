{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2j-51uaxvLSd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA CLEANINING NOTEBOOK**"
      ],
      "metadata": {
        "id": "SLOF5WCVYWd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install beautifulsoup4 nltk"
      ],
      "metadata": {
        "id": "gibK7zeII0qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CLEANING**"
      ],
      "metadata": {
        "id": "DOd65OKDI4w4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Function to clean the text (remove special characters, etc.)\n",
        "def clean_text(text):\n",
        "    print(\"ðŸ§¹ Cleaning the text...\")\n",
        "    # Removing any non-ASCII characters\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII chars\n",
        "    text = re.sub(r\"[^a-zA-Z0-9.,!?/:;'â€™\\-\\s]\", ' ', text)  # Remove unwanted characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize multiple spaces\n",
        "    return text\n",
        "\n",
        "# Function to fix incomplete sentences\n",
        "def fix_incomplete_sentences(text):\n",
        "    print(\"ðŸ§  Fixing fragmented sentences...\")\n",
        "    # Basic sentence splitting using punctuation as delimiter\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    fixed_sentences = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(sentences):\n",
        "        sentence = sentences[i].strip()\n",
        "        # If the sentence is too short, try to combine with the next one\n",
        "        if len(sentence.split()) < 4 or not sentence.endswith(('.', '?', '!')):\n",
        "            if i + 1 < len(sentences):\n",
        "                sentence += ' ' + sentences[i + 1]\n",
        "                i += 1  # Skip next one\n",
        "        fixed_sentences.append(sentence)\n",
        "        i += 1\n",
        "\n",
        "    return ' '.join(fixed_sentences)\n",
        "\n",
        "# Batch processing function for large JSONL files\n",
        "def process_jsonl_in_batches(input_file, output_file, batch_size=1000):\n",
        "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        batch = []\n",
        "        for i, line in enumerate(infile, 1):\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"âš ï¸ Error decoding line {i}: {e}\")\n",
        "                continue  # Skip invalid JSON lines\n",
        "\n",
        "            original_text = data.get(\"text\", \"\")\n",
        "\n",
        "            if original_text:\n",
        "                cleaned_text = clean_text(original_text)\n",
        "                completed_text = fix_incomplete_sentences(cleaned_text)\n",
        "                data[\"text\"] = completed_text\n",
        "\n",
        "            batch.append(data)\n",
        "\n",
        "            # Write the batch to the output file when batch size is reached\n",
        "            if len(batch) >= batch_size:\n",
        "                for item in batch:\n",
        "                    outfile.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "                batch.clear()  # Clear the batch\n",
        "                print(f\"âœ… Batch {i//batch_size} processed...\")\n",
        "\n",
        "        # Write any remaining data in the last batch\n",
        "        if batch:\n",
        "            for item in batch:\n",
        "                outfile.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    print(f\"\\nðŸŽ‰ Batch processing complete! Output saved to: {output_file}\")\n",
        "\n",
        "# === Prompt for file path ===\n",
        "input_file_path = input(\"ðŸ“‚ Enter the path to your .jsonl file: \")\n",
        "output_file_path = input(\"ðŸ“‚ Enter the path for the output .jsonl file: \")\n",
        "\n",
        "if not os.path.exists(input_file_path):\n",
        "    print(\"âŒ Input file not found!\")\n",
        "    exit()\n",
        "\n",
        "print(\"\\nðŸš€ Starting batch processing...\\n\")\n",
        "time.sleep(1)\n",
        "\n",
        "process_jsonl_in_batches(input_file_path, output_file_path)\n"
      ],
      "metadata": {
        "id": "fFUCaNozI7nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lqzdOoRmYex1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CLEANING WITH LLM MODEL**"
      ],
      "metadata": {
        "id": "EMcrEsn8YfsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "XyG9jiO6Jy96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the model (you can replace with a different one)\n",
        "model_name = \"google/flan-t5-base\"  # lightweight, works well for generation tasks\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Function to clean/clarify a batch of text\n",
        "def clarify_batch(texts, max_input_length=256, max_output_length=256):\n",
        "    prompts = [f\"Fix and rephrase the following educational statement: {t}\" for t in texts]\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_length).to(device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_output_length)\n",
        "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return decoded\n",
        "\n",
        "# === File Paths ===\n",
        "input_file = input(\"ðŸ“‚ Enter path to your .jsonl file: \").strip()\n",
        "output_file = input(\"ðŸ’¾ Enter output path: \").strip()\n",
        "\n",
        "batch_size = 8  # can increase based on your system\n",
        "\n",
        "# === Processing ===\n",
        "print(\"\\nðŸ¦¾ Loading and cleaning using LLM...\\n\")\n",
        "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "    batch = []\n",
        "    lines = []\n",
        "\n",
        "    for line in tqdm(infile, desc=\"ðŸš€ Processing\"):\n",
        "        try:\n",
        "            data = json.loads(line)\n",
        "            text = data.get(\"text\", \"\")\n",
        "            if text:\n",
        "                batch.append(text)\n",
        "                lines.append(data)\n",
        "        except json.JSONDecodeError:\n",
        "            continue  # Skip bad lines\n",
        "\n",
        "        if len(batch) >= batch_size:\n",
        "            clarified = clarify_batch(batch)\n",
        "            for i in range(len(clarified)):\n",
        "                lines[i]['text'] = clarified[i]\n",
        "                outfile.write(json.dumps(lines[i], ensure_ascii=False) + '\\n')\n",
        "            batch, lines = [], []\n",
        "\n",
        "    # Final remaining batch\n",
        "    if batch:\n",
        "        clarified = clarify_batch(batch)\n",
        "        for i in range(len(clarified)):\n",
        "            lines[i]['text'] = clarified[i]\n",
        "            outfile.write(json.dumps(lines[i], ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(\"\\nâœ… LLM-based text refinement complete! Output saved to:\", output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fCe_VedLvo3",
        "outputId": "c2d54779-05ad-44a7-fbe2-23ebc5803618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ðŸš€ Processing: 1216it [54:25,  3.18s/it]"
          ]
        }
      ]
    }
  ]
}