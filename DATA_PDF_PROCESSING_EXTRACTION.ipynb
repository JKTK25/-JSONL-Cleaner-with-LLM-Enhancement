{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMUazerqLzL7aEJR94NYXx/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JKTK25/-JSONL-Cleaner-with-LLM-Enhancement/blob/main/DATA_PDF_PROCESSING_EXTRACTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SX9FjgodxDI"
      },
      "outputs": [],
      "source": [
        "pip install PyPDF2 tiktoken pytesseract Pillow langdetect pymupdf docx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyMuPDF langdetect"
      ],
      "metadata": {
        "id": "gpkUDgn3unmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9ZJGYGHu-Gq",
        "outputId": "27f41fea-1edb-4408-a3c2-583a7197b447"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EXTRACTING DATA FOR MACHINE LEARNING**"
      ],
      "metadata": {
        "id": "iHEFBl3kgrKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA PDF PROCESSING EXTRACTION**\n"
      ],
      "metadata": {
        "id": "s7tRKERkVCk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from typing import List, Optional, Dict, Tuple\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import fitz  # PyMuPDF\n",
        "import langdetect\n",
        "from langdetect import DetectorFactory\n",
        "import logging\n",
        "from docx import Document\n",
        "import os\n",
        "import concurrent.futures\n",
        "from functools import partial\n",
        "\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "    USE_TQDM = True\n",
        "except ImportError:\n",
        "    USE_TQDM = False\n",
        "\n",
        "# Try importing tkinter, but allow fallback\n",
        "try:\n",
        "    import tkinter as tk\n",
        "    from tkinter import filedialog\n",
        "    TKINTER_AVAILABLE = True\n",
        "except (ImportError, RuntimeError):\n",
        "    TKINTER_AVAILABLE = False\n",
        "\n",
        "# Consistent language detection\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
        "\n",
        "class EnhancedDocumentExtractor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dir: Optional[str] = None,\n",
        "        output_file: Optional[str] = None,\n",
        "        max_tokens: Optional[int] = 512,\n",
        "        min_chunk_size: int = 100,\n",
        "        overlap: int = 50,\n",
        "        target_language: Optional[str] = \"en\",\n",
        "        min_text_quality: float = 0.7,\n",
        "        enable_ocr: bool = True,\n",
        "        num_workers: int = None\n",
        "    ):\n",
        "        # Default directories\n",
        "        default_input_dir = \"./documents\"\n",
        "        default_output_file = \"./pretraining_data.jsonl\"\n",
        "\n",
        "        # Handle input directory selection\n",
        "        if input_dir:\n",
        "            self.input_dir = Path(input_dir)\n",
        "        else:\n",
        "            if TKINTER_AVAILABLE and os.environ.get('DISPLAY'):\n",
        "                try:\n",
        "                    root = tk.Tk()\n",
        "                    root.withdraw()\n",
        "                    self.input_dir = Path(filedialog.askdirectory(title=\"Select Input Directory\") or default_input_dir)\n",
        "                    root.destroy()\n",
        "                except Exception:\n",
        "                    logging.warning(\"Falling back to console input for directory selection\")\n",
        "                    self.input_dir = self._prompt_directory(\"input directory\", default_input_dir)\n",
        "            else:\n",
        "                self.input_dir = self._prompt_directory(\"input directory\", default_input_dir)\n",
        "\n",
        "        if not self.input_dir.exists():\n",
        "            raise ValueError(f\"Input directory {self.input_dir} does not exist\")\n",
        "\n",
        "        # Handle output file selection\n",
        "        if output_file:\n",
        "            self.output_file = Path(output_file)\n",
        "        else:\n",
        "            if TKINTER_AVAILABLE and os.environ.get('DISPLAY'):\n",
        "                try:\n",
        "                    root = tk.Tk()\n",
        "                    root.withdraw()\n",
        "                    selected_file = filedialog.asksaveasfilename(\n",
        "                        title=\"Select Output File\",\n",
        "                        defaultextension=\".jsonl\",\n",
        "                        filetypes=[(\"JSON Lines\", \"*.jsonl\"), (\"All files\", \"*.*\")]\n",
        "                    )\n",
        "                    self.output_file = Path(selected_file or default_output_file)\n",
        "                    root.destroy()\n",
        "                except Exception:\n",
        "                    logging.warning(\"Falling back to console input for output file selection\")\n",
        "                    self.output_file = self._prompt_directory(\"output file\", default_output_file, is_file=True)\n",
        "            else:\n",
        "                self.output_file = self._prompt_directory(\"output file\", default_output_file, is_file=True)\n",
        "\n",
        "        self.max_tokens = max_tokens\n",
        "        self.min_chunk_size = min_chunk_size\n",
        "        self.overlap = overlap\n",
        "        self.target_language = target_language\n",
        "        self.min_text_quality = min_text_quality\n",
        "        self.enable_ocr = enable_ocr\n",
        "        self.num_workers = num_workers or os.cpu_count() // 2 or 1\n",
        "        self.seen_hashes = self._load_existing_hashes()\n",
        "        self._setup_encoder()\n",
        "\n",
        "    def _setup_encoder(self):\n",
        "        \"\"\"Lazy load the tokenizer only when needed\"\"\"\n",
        "        if self.max_tokens:\n",
        "            import tiktoken\n",
        "            self.encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
        "        else:\n",
        "            self.encoder = None\n",
        "\n",
        "    def _prompt_directory(self, dir_type: str, default: str, is_file: bool = False) -> Path:\n",
        "        \"\"\"Prompt user for directory or file path via console with a default option.\"\"\"\n",
        "        prompt = f\"Enter {dir_type} (default: {default}): \"\n",
        "        user_input = input(prompt).strip()\n",
        "        path = Path(user_input or default)\n",
        "\n",
        "        if is_file:\n",
        "            # Ensure parent directory exists for output file\n",
        "            path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        else:\n",
        "            # Ensure input directory exists\n",
        "            path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        return path\n",
        "\n",
        "    def _load_existing_hashes(self) -> set:\n",
        "        \"\"\"Prevent duplicate processing by loading already written hashes from output file.\"\"\"\n",
        "        seen = set()\n",
        "        if self.output_file.exists():\n",
        "            logging.info(\"Resuming from previous output...\")\n",
        "            with open(self.output_file, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    try:\n",
        "                        record = json.loads(line)\n",
        "                        text = record.get(\"text\", \"\")\n",
        "                        hash_ = self.generate_content_hash(text)\n",
        "                        seen.add(hash_)\n",
        "                    except Exception:\n",
        "                        continue\n",
        "        return seen\n",
        "\n",
        "    def calculate_text_quality(self, text: str) -> float:\n",
        "        \"\"\"Efficient text quality calculation using precomputed values\"\"\"\n",
        "        total_chars = len(text)\n",
        "        if total_chars == 0:\n",
        "            return 0.0\n",
        "\n",
        "        alpha_count = sum(1 for c in text if c.isalpha())\n",
        "        alpha_ratio = alpha_count / total_chars\n",
        "\n",
        "        words = re.findall(r'\\b\\w+\\b', text)\n",
        "        word_count = len(words)\n",
        "        if word_count == 0:\n",
        "            return 0.0\n",
        "\n",
        "        valid_words = sum(1 for word in words if word.isalpha())\n",
        "        word_ratio = valid_words / word_count\n",
        "\n",
        "        special_chars = len(re.findall(r'[^\\w\\s]', text))\n",
        "        special_ratio = 1 - (special_chars / total_chars)\n",
        "\n",
        "        return (alpha_ratio * 0.4 + word_ratio * 0.4 + special_ratio * 0.2)\n",
        "\n",
        "    def detect_language(self, text: str) -> Optional[str]:\n",
        "        \"\"\"Efficient language detection using sampling\"\"\"\n",
        "        sample_text = text[:2000]  # Use first 2000 characters for detection\n",
        "        if len(sample_text.strip()) < 10:\n",
        "            return None\n",
        "        try:\n",
        "            return langdetect.detect(sample_text)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def generate_content_hash(self, text: str) -> str:\n",
        "        normalized = re.sub(r'\\s+', ' ', text).strip().lower()\n",
        "        return hashlib.md5(normalized.encode('utf-8')).hexdigest()\n",
        "\n",
        "    def is_duplicate(self, text: str) -> bool:\n",
        "        content_hash = self.generate_content_hash(text)\n",
        "        return content_hash in self.seen_hashes\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Optimized text cleaning with compiled regex patterns\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Compile regex patterns once\n",
        "        if not hasattr(self, 'clean_patterns'):\n",
        "            self.clean_patterns = {\n",
        "                'whitespace': re.compile(r'\\s+'),\n",
        "                'hyphen_newline': re.compile(r'-\\n'),\n",
        "                'newline': re.compile(r'\\n'),\n",
        "                'control_chars': re.compile(r'[\\x00-\\x1f\\x7f-\\x9f]'),\n",
        "                'form_feed': re.compile(r'\\x0c'),\n",
        "                'header_footer': re.compile(r'^\\d+\\s+\\w+\\s+\\d+$', flags=re.MULTILINE)\n",
        "            }\n",
        "\n",
        "        text = self.clean_patterns['hyphen_newline'].sub('', text)\n",
        "        text = self.clean_patterns['header_footer'].sub('', text)\n",
        "        text = self.clean_patterns['control_chars'].sub('', text)\n",
        "        text = self.clean_patterns['form_feed'].sub('', text)\n",
        "        text = self.clean_patterns['newline'].sub(' ', text)\n",
        "        text = self.clean_patterns['whitespace'].sub(' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def extract_pdf_text(self, pdf_path: Path) -> Tuple[str, bool]:\n",
        "        \"\"\"Extract text from PDF using PyMuPDF with OCR fallback per page\"\"\"\n",
        "        text = \"\"\n",
        "        is_ocr = False\n",
        "\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            for page in doc:\n",
        "                # First try text extraction\n",
        "                page_text = page.get_text()\n",
        "                if page_text.strip() and len(page_text) > 50:  # Valid text\n",
        "                    text += self.clean_text(page_text) + \"\\n\"\n",
        "                elif self.enable_ocr:\n",
        "                    # Use OCR only for this page\n",
        "                    pix = page.get_pixmap()\n",
        "                    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "                    ocr_text = pytesseract.image_to_string(img)\n",
        "                    cleaned_ocr = self.clean_text(ocr_text)\n",
        "                    if cleaned_ocr:\n",
        "                        text += cleaned_ocr + \"\\n\"\n",
        "                        is_ocr = True\n",
        "            doc.close()\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"PDF processing failed for {pdf_path.name}: {e}\")\n",
        "            if self.enable_ocr:\n",
        "                logging.info(f\"Attempting full OCR for {pdf_path.name}\")\n",
        "                try:\n",
        "                    doc = fitz.open(pdf_path)\n",
        "                    for page in doc:\n",
        "                        pix = page.get_pixmap()\n",
        "                        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "                        ocr_text = pytesseract.image_to_string(img)\n",
        "                        text += self.clean_text(ocr_text) + \"\\n\"\n",
        "                    is_ocr = True\n",
        "                except Exception as e2:\n",
        "                    logging.warning(f\"OCR failed for {pdf_path.name}: {e2}\")\n",
        "\n",
        "        return text, is_ocr\n",
        "\n",
        "    def extract_docx_text(self, docx_path: Path) -> Tuple[str, bool]:\n",
        "        text = \"\"\n",
        "        try:\n",
        "            doc = Document(docx_path)\n",
        "            for para in doc.paragraphs:\n",
        "                if para.text.strip():\n",
        "                    text += self.clean_text(para.text) + \"\\n\"\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Failed to process {docx_path.name}: {e}\")\n",
        "        return text, False\n",
        "\n",
        "    def extract_text(self, file_path: Path) -> Tuple[str, bool, str]:\n",
        "        ext = file_path.suffix.lower()\n",
        "        if ext == '.pdf':\n",
        "            text, is_ocr = self.extract_pdf_text(file_path)\n",
        "            return text, is_ocr, 'pdf'\n",
        "        elif ext in ('.docx', '.doc'):\n",
        "            text, is_ocr = self.extract_docx_text(file_path)\n",
        "            return text, is_ocr, 'docx'\n",
        "        return \"\", False, 'unknown'\n",
        "\n",
        "    def chunk_text(self, text: str) -> List[str]:\n",
        "        \"\"\"Efficient text chunking with token-based boundaries\"\"\"\n",
        "        if not self.max_tokens or not text.strip():\n",
        "            return [text] if text.strip() else []\n",
        "\n",
        "        # Split into paragraphs first\n",
        "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_token_count = 0\n",
        "\n",
        "        for para in paragraphs:\n",
        "            para_tokens = self.encoder.encode(para) if self.encoder else []\n",
        "            para_token_count = len(para_tokens)\n",
        "\n",
        "            # If paragraph is too big, split into sentences\n",
        "            if para_token_count > self.max_tokens:\n",
        "                sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
        "                for sentence in sentences:\n",
        "                    if not sentence.strip():\n",
        "                        continue\n",
        "                    sent_tokens = self.encoder.encode(sentence) if self.encoder else []\n",
        "                    sent_token_count = len(sent_tokens)\n",
        "\n",
        "                    # Add sentence to current chunk if it fits\n",
        "                    if current_token_count + sent_token_count <= self.max_tokens:\n",
        "                        current_chunk.append(sentence)\n",
        "                        current_token_count += sent_token_count\n",
        "                    else:\n",
        "                        # Finalize current chunk\n",
        "                        if current_chunk:\n",
        "                            chunk_text = ' '.join(current_chunk)\n",
        "                            if len(chunk_text) >= self.min_chunk_size:\n",
        "                                chunks.append(chunk_text)\n",
        "\n",
        "                            # Start new chunk with overlap\n",
        "                            overlap_sents = current_chunk[-min(len(current_chunk), 3):]  # Last 1-3 sentences\n",
        "                            current_chunk = overlap_sents + [sentence]\n",
        "                            current_token_count = sum(len(self.encoder.encode(s)) for s in current_chunk)\n",
        "                        else:\n",
        "                            current_chunk = [sentence]\n",
        "                            current_token_count = sent_token_count\n",
        "            else:\n",
        "                # Add entire paragraph to current chunk\n",
        "                if current_token_count + para_token_count <= self.max_tokens:\n",
        "                    current_chunk.append(para)\n",
        "                    current_token_count += para_token_count\n",
        "                else:\n",
        "                    # Finalize current chunk\n",
        "                    if current_chunk:\n",
        "                        chunk_text = '\\n\\n'.join(current_chunk)\n",
        "                        if len(chunk_text) >= self.min_chunk_size:\n",
        "                            chunks.append(chunk_text)\n",
        "\n",
        "                        # Start new chunk with overlap\n",
        "                        overlap_paras = [current_chunk[-1]] if current_chunk else []\n",
        "                        current_chunk = overlap_paras + [para]\n",
        "                        current_token_count = para_token_count + (len(self.encoder.encode(overlap_paras[0])) if overlap_paras else 0)\n",
        "                    else:\n",
        "                        current_chunk = [para]\n",
        "                        current_token_count = para_token_count\n",
        "\n",
        "        # Add final chunk\n",
        "        if current_chunk:\n",
        "            chunk_text = '\\n\\n'.join(current_chunk)\n",
        "            if len(chunk_text) >= self.min_chunk_size:\n",
        "                chunks.append(chunk_text)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def process_single_file(self, file_path: Path, stats: Dict[str, int]) -> List[Dict]:\n",
        "        \"\"\"Process a single file and return chunks\"\"\"\n",
        "        try:\n",
        "            text, is_ocr, file_type = self.extract_text(file_path)\n",
        "            if not text.strip():\n",
        "                return []\n",
        "\n",
        "            # Update file type stats\n",
        "            if file_type == 'pdf':\n",
        "                stats[\"pdf_files\"] += 1\n",
        "            elif file_type == 'docx':\n",
        "                stats[\"docx_files\"] += 1\n",
        "\n",
        "            # Language detection\n",
        "            lang = None\n",
        "            if self.target_language:\n",
        "                lang = self.detect_language(text)\n",
        "                if lang != self.target_language:\n",
        "                    stats[\"language_filtered\"] += 1\n",
        "                    return []\n",
        "\n",
        "            # Chunk text\n",
        "            chunks = self.chunk_text(text)\n",
        "            records = []\n",
        "\n",
        "            for chunk in chunks:\n",
        "                # Skip duplicates and low-quality chunks\n",
        "                if self.is_duplicate(chunk):\n",
        "                    stats[\"duplicates_removed\"] += 1\n",
        "                    continue\n",
        "\n",
        "                quality_score = self.calculate_text_quality(chunk)\n",
        "                if quality_score < self.min_text_quality:\n",
        "                    continue\n",
        "\n",
        "                # Generate record\n",
        "                records.append({\n",
        "                    \"text\": chunk,\n",
        "                    \"metadata\": {\n",
        "                        \"source\": file_path.name,\n",
        "                        \"file_type\": file_type,\n",
        "                        \"language\": lang,\n",
        "                        \"quality_score\": quality_score,\n",
        "                        \"ocr_used\": is_ocr,\n",
        "                        \"chunk_length\": len(chunk),\n",
        "                        \"token_count\": len(self.encoder.encode(chunk)) if self.encoder else None\n",
        "                    }\n",
        "                })\n",
        "\n",
        "                # Update seen hashes\n",
        "                content_hash = self.generate_content_hash(chunk)\n",
        "                self.seen_hashes.add(content_hash)\n",
        "\n",
        "            if records:\n",
        "                stats[\"processed\"] += 1\n",
        "                if is_ocr:\n",
        "                    stats[\"ocr_used\"] += 1\n",
        "\n",
        "            return records\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing {file_path.name}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def process_documents(self):\n",
        "        # Collect files\n",
        "        pdf_files = list(self.input_dir.rglob(\"*.pdf\"))\n",
        "        docx_files = list(self.input_dir.rglob(\"*.docx\"))\n",
        "        files = pdf_files + docx_files\n",
        "        if not files:\n",
        "            logging.warning(f\"No PDF or Word files found in {self.input_dir}\")\n",
        "            return\n",
        "\n",
        "        stats = {\n",
        "            \"total_files\": len(files),\n",
        "            \"processed\": 0,\n",
        "            \"language_filtered\": 0,\n",
        "            \"duplicates_removed\": 0,\n",
        "            \"ocr_used\": 0,\n",
        "            \"pdf_files\": 0,\n",
        "            \"docx_files\": 0\n",
        "        }\n",
        "\n",
        "        # Prepare output file\n",
        "        self.output_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Parallel processing with thread pool\n",
        "        with open(self.output_file, 'a', encoding='utf-8') as outfile:\n",
        "            if self.num_workers > 1:\n",
        "                with concurrent.futures.ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n",
        "                    # Process files in parallel\n",
        "                    future_to_file = {\n",
        "                        executor.submit(self.process_single_file, file, stats): file\n",
        "                        for file in files\n",
        "                    }\n",
        "\n",
        "                    # Create progress bar\n",
        "                    iterator = tqdm(\n",
        "                        concurrent.futures.as_completed(future_to_file),\n",
        "                        total=len(files),\n",
        "                        desc=\"Processing Documents\"\n",
        "                    ) if USE_TQDM else concurrent.futures.as_completed(future_to_file)\n",
        "\n",
        "                    # Collect results\n",
        "                    for future in iterator:\n",
        "                        records = future.result()\n",
        "                        for record in records:\n",
        "                            outfile.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
        "            else:\n",
        "                # Sequential processing\n",
        "                iterator = tqdm(files, desc=\"Processing Documents\") if USE_TQDM else files\n",
        "                for file in iterator:\n",
        "                    records = self.process_single_file(file, stats)\n",
        "                    for record in records:\n",
        "                        outfile.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        logging.info(\"\\n‚úÖ Processing Complete\")\n",
        "        for key, val in stats.items():\n",
        "            logging.info(f\"- {key.replace('_', ' ').capitalize()}: {val}\")\n",
        "        logging.info(f\"\\nüìù Output saved to: {self.output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    extractor = EnhancedDocumentExtractor(\n",
        "        max_tokens=512,\n",
        "        min_chunk_size=100,\n",
        "        overlap=50,\n",
        "        target_language=\"en\",\n",
        "        min_text_quality=0.65,\n",
        "        enable_ocr=True,\n",
        "        num_workers=4  # Optimal for I/O bound tasks\n",
        "    )\n",
        "    extractor.process_documents()"
      ],
      "metadata": {
        "id": "ltziFXFyd6_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78c13dfd-0fda-4043-f2db-ca5134793212"
      },
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter input directory (default: ./documents): /content/phyctex\n",
            "Enter output file (default: ./pretraining_data.jsonl): /content/phyctex/physic2.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [04:29<00:00, 53.88s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CLEANING DATA FOR GOOD SMART**"
      ],
      "metadata": {
        "id": "iTwtriJzFrKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INSTALLING**"
      ],
      "metadata": {
        "id": "ob4DNd4eQr3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRYING**"
      ],
      "metadata": {
        "id": "jamKSScoX9Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "import threading\n",
        "import hashlib\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "from language_tool_python import LanguageTool\n",
        "\n",
        "# Global thread-local storage for resources\n",
        "thread_local = threading.local()\n",
        "\n",
        "class TextCleaner:\n",
        "    def __init__(self, enable_grammar=False):\n",
        "        \"\"\"Initialize with models for Colab\"\"\"\n",
        "        self.enable_grammar = enable_grammar\n",
        "        self.nlp = None\n",
        "        self.tool = None\n",
        "\n",
        "        # Precompile regex patterns for efficiency\n",
        "        self.patterns = {\n",
        "            'non_ascii': re.compile(r'[^\\x00-\\x7F]+'),\n",
        "            'zero_width': re.compile(r'[\\u200b-\\u200d\\uFEFF]'),\n",
        "            'control_chars': re.compile(r'[\\x00-\\x1f\\x7f-\\x9f]'),\n",
        "            'quotes': re.compile(r'[‚Äò‚Äô]'),\n",
        "            'double_quotes': re.compile(r'[‚Äú‚Äù]'),\n",
        "            'symbols': re.compile(r\"[^a-zA-Z0-9\\s.,!?;:'\\\"-]\"),\n",
        "            'whitespace': re.compile(r'\\s+'),\n",
        "            'repeated_words': re.compile(r'\\b(\\w+)\\s+\\1\\b', re.I),\n",
        "            'sentence_end': re.compile(r'[.!?]$'),\n",
        "            'sentence_start': re.compile(r'^\\s*[a-z]')\n",
        "        }\n",
        "\n",
        "    def _load_models(self):\n",
        "        \"\"\"Lazy load models when needed with proper pipeline setup\"\"\"\n",
        "        if self.nlp is None:\n",
        "            # Load with minimal components and add sentencizer\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"lemmatizer\", \"tagger\"])\n",
        "            if \"sentencizer\" not in self.nlp.pipe_names:\n",
        "                self.nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "        if self.tool is None and self.enable_grammar:\n",
        "            self.tool = LanguageTool('en-US', config={'cacheSize': 1000, 'pipelineCaching': True})\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Optimized text cleaning with precompiled regex\"\"\"\n",
        "        if not text or not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Apply regex substitutions in sequence\n",
        "        text = self.patterns['non_ascii'].sub(' ', text)\n",
        "        text = self.patterns['zero_width'].sub('', text)\n",
        "        text = self.patterns['control_chars'].sub('', text)\n",
        "        text = self.patterns['quotes'].sub(\"'\", text)\n",
        "        text = self.patterns['double_quotes'].sub('\"', text)\n",
        "        text = self.patterns['symbols'].sub(' ', text)\n",
        "        text = self.patterns['whitespace'].sub(' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def fix_grammar(self, text):\n",
        "        \"\"\"Grammar correction with caching and length limits\"\"\"\n",
        "        if not self.enable_grammar or len(text.split()) < 3 or len(text) > 5000:\n",
        "            return text\n",
        "\n",
        "        try:\n",
        "            # Use caching and limit text size for performance\n",
        "            cache_key = hashlib.md5(text.encode()).hexdigest()\n",
        "            if not hasattr(self, 'grammar_cache'):\n",
        "                self.grammar_cache = {}\n",
        "\n",
        "            if cache_key in self.grammar_cache:\n",
        "                return self.grammar_cache[cache_key]\n",
        "\n",
        "            matches = self.tool.check(text)\n",
        "            if matches:\n",
        "                corrected = self.tool.correct(text)\n",
        "                self.grammar_cache[cache_key] = corrected\n",
        "                return corrected\n",
        "            return text\n",
        "        except:\n",
        "            return text\n",
        "\n",
        "    def fix_sentences(self, text):\n",
        "        \"\"\"Efficient sentence structure improvement with fallback\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        self._load_models()\n",
        "        if not self.nlp:\n",
        "            return text\n",
        "\n",
        "        try:\n",
        "            # Process in chunks for long texts\n",
        "            if len(text) > 10000:\n",
        "                chunks = [text[i:i+10000] for i in range(0, len(text), 10000)]\n",
        "                return \" \".join(self.fix_sentences(chunk) for chunk in chunks)\n",
        "\n",
        "            doc = self.nlp(text)\n",
        "            sentences = []\n",
        "\n",
        "            for sent in doc.sents:\n",
        "                sent_text = sent.text.strip()\n",
        "                if not sent_text:\n",
        "                    continue\n",
        "\n",
        "                # Efficient sentence formatting\n",
        "                if not self.patterns['sentence_end'].search(sent_text):\n",
        "                    sent_text += '.'\n",
        "                if self.patterns['sentence_start'].search(sent_text):\n",
        "                    sent_text = sent_text[0].upper() + sent_text[1:]\n",
        "\n",
        "                sentences.append(sent_text)\n",
        "\n",
        "            return ' '.join(sentences)\n",
        "        except Exception as e:\n",
        "            print(f\"Sentence fixing failed: {e}\")\n",
        "            # Fallback to simple sentence splitting\n",
        "            sentences = []\n",
        "            for line in re.split(r'(?<=[.!?])\\s+', text):\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    if line[0].islower():\n",
        "                        line = line[0].upper() + line[1:]\n",
        "                    if not line.endswith(('.', '!', '?')):\n",
        "                        line += '.'\n",
        "                    sentences.append(line)\n",
        "            return ' '.join(sentences)\n",
        "\n",
        "    def check_quality(self, text):\n",
        "        \"\"\"Fast quality checks with early termination\"\"\"\n",
        "        if not text.strip():\n",
        "            return False\n",
        "\n",
        "        words = text.split()\n",
        "        word_count = len(words)\n",
        "\n",
        "        if word_count < 5:\n",
        "            return False\n",
        "        if len(set(map(str.lower, words))) < 3:\n",
        "            return False\n",
        "        if self.patterns['repeated_words'].search(text):\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def process_text(self, text):\n",
        "        \"\"\"Optimized text processing pipeline with better error handling\"\"\"\n",
        "        if not text or not isinstance(text, str):\n",
        "            return None\n",
        "\n",
        "        cleaned = self.clean_text(text)\n",
        "\n",
        "        # Early termination for low-quality text\n",
        "        if not self.check_quality(cleaned):\n",
        "            return None\n",
        "\n",
        "        if self.enable_grammar:\n",
        "            try:\n",
        "                cleaned = self.fix_grammar(cleaned)\n",
        "            except Exception as e:\n",
        "                print(f\"Grammar fixing failed: {e}\")\n",
        "\n",
        "        try:\n",
        "            cleaned = self.fix_sentences(cleaned)\n",
        "        except Exception as e:\n",
        "            print(f\"Sentence fixing failed: {e}\")\n",
        "\n",
        "        return cleaned if self.check_quality(cleaned) else None\n",
        "\n",
        "def init_worker(enable_grammar):\n",
        "    \"\"\"Initialize thread-local resources\"\"\"\n",
        "    thread_local.cleaner = TextCleaner(enable_grammar)\n",
        "\n",
        "def process_line(line):\n",
        "    \"\"\"Process a single line with thread-local cleaner\"\"\"\n",
        "    try:\n",
        "        record = json.loads(line)\n",
        "        if 'text' in record and record['text']:\n",
        "            cleaned = thread_local.cleaner.process_text(record['text'])\n",
        "            if cleaned:\n",
        "                record['text'] = cleaned\n",
        "                record['cleaned_at'] = time.time()\n",
        "                return record\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing line: {e}\")\n",
        "        return None\n",
        "\n",
        "def clean_jsonl(input_path, output_path, enable_grammar=False, num_workers=4):\n",
        "    \"\"\"Optimized JSONL processing with parallel execution\"\"\"\n",
        "    # Count total lines for progress bar\n",
        "    total_lines = 0\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        total_lines = sum(1 for _ in f)\n",
        "\n",
        "    processed_count = 0\n",
        "    skipped_count = 0\n",
        "\n",
        "    with open(input_path, 'r', encoding='utf-8') as infile, \\\n",
        "         open(output_path, 'w', encoding='utf-8') as outfile:\n",
        "\n",
        "        # Create a list of all lines for processing\n",
        "        lines = infile.readlines()\n",
        "\n",
        "        with ThreadPoolExecutor(\n",
        "            max_workers=num_workers,\n",
        "            initializer=init_worker,\n",
        "            initargs=(enable_grammar,)\n",
        "        ) as executor:\n",
        "\n",
        "            # Process all lines with tqdm progress bar\n",
        "            results = []\n",
        "            with tqdm(total=total_lines, desc=\"Processing Records\") as pbar:\n",
        "                for result in executor.map(process_line, lines):\n",
        "                    if result:\n",
        "                        outfile.write(json.dumps(result) + '\\n')\n",
        "                        processed_count += 1\n",
        "                    else:\n",
        "                        skipped_count += 1\n",
        "                    pbar.update(1)\n",
        "\n",
        "    print(f\"\\nProcessed: {processed_count} records\")\n",
        "    print(f\"Skipped: {skipped_count} records\")\n",
        "    return processed_count, skipped_count\n",
        "\n",
        "# For Google Colab file handling\n",
        "from google.colab import files\n",
        "\n",
        "def main():\n",
        "    print(\"üßº Optimized Text Cleaner for Colab\")\n",
        "\n",
        "    # Configuration\n",
        "    ENABLE_GRAMMAR = False  # Disabled by default due to performance impact\n",
        "    NUM_WORKERS = 4          # Optimal for Colab environment\n",
        "\n",
        "    # File handling\n",
        "    input_file = 'input.jsonl'\n",
        "    if not os.path.exists(input_file):\n",
        "        print(\"Please upload your input file:\")\n",
        "        uploaded = files.upload()\n",
        "        if uploaded:\n",
        "            input_file = next(iter(uploaded.keys()))\n",
        "        else:\n",
        "            print(\"No file uploaded!\")\n",
        "            return\n",
        "\n",
        "    output_file = 'cleaned_output.jsonl'\n",
        "\n",
        "    print(\"\\n‚ö° Processing your data...\")\n",
        "    start = time.time()\n",
        "\n",
        "    # Process the file\n",
        "    processed_count, skipped_count = clean_jsonl(\n",
        "        input_file,\n",
        "        output_file,\n",
        "        enable_grammar=ENABLE_GRAMMAR,\n",
        "        num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    duration = time.time() - start\n",
        "    rate = processed_count / duration if duration > 0 else 0\n",
        "\n",
        "    print(f\"\\n‚úÖ Cleaning complete in {duration:.2f} seconds\")\n",
        "    print(f\"Processing rate: {rate:.1f} records/sec\")\n",
        "\n",
        "    # Verify output\n",
        "    output_size = os.path.getsize(output_file)\n",
        "    if output_size == 0:\n",
        "        print(\"\\n‚ö†Ô∏è Warning: Output file is empty!\")\n",
        "        print(\"Possible reasons:\")\n",
        "        print(\"- All records were filtered out by quality checks\")\n",
        "        print(\"- Input file format is invalid\")\n",
        "        print(\"- No 'text' field found in input records\")\n",
        "        print(\"- Text cleaning removed all content\")\n",
        "\n",
        "        # Debug: Show first 5 records\n",
        "        print(\"\\nDebugging first 5 records:\")\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i >= 5:\n",
        "                    break\n",
        "                try:\n",
        "                    record = json.loads(line)\n",
        "                    text = record.get('text', '')\n",
        "                    print(f\"Record {i+1}:\")\n",
        "                    print(f\"  Original length: {len(text)}\")\n",
        "                    print(f\"  First 100 chars: {text[:100]}\")\n",
        "                except:\n",
        "                    print(f\"Record {i+1}: Invalid JSON\")\n",
        "    else:\n",
        "        print(f\"Output file size: {output_size/1024:.1f} KB\")\n",
        "        print(\"Download your cleaned file:\")\n",
        "        files.download(output_file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "eUr6mSv2YAkn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "648cd01b-5058-41ab-f2eb-e9430272649e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßº Optimized Text Cleaner for Colab\n",
            "Please upload your input file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6b8db1d1-3959-43db-b4b4-1f0f1cff078b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6b8db1d1-3959-43db-b4b4-1f0f1cff078b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving physic2(1).jsonl to physic2(1).jsonl\n",
            "\n",
            "‚ö° Processing your data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Records: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 530/530 [00:11<00:00, 45.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processed: 434 records\n",
            "Skipped: 96 records\n",
            "\n",
            "‚úÖ Cleaning complete in 11.74 seconds\n",
            "Processing rate: 37.0 records/sec\n",
            "Output file size: 952.2 KB\n",
            "Download your cleaned file:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c1567508-84c4-4f09-8c6a-0daa2bc69dd1\", \"cleaned_output.jsonl\", 975045)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf3a4611"
      },
      "source": [
        "!pip install python-docx language_tool_python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4b94730"
      },
      "source": [
        "!pip install language_tool_python spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f32b837"
      },
      "source": [
        "!pip install language_tool_python spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75e80431"
      },
      "source": [
        "!pip install pytesseract"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}